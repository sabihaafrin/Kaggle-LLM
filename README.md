# Dataset : 100 LLM Papers to explore
 Large language models are continually improving, with greater task efficiency and higher accuracy. Hence, it's necessary to stay up to date on current research findings that describe novel models and frameworks. The Kaggle dataset 100-llm-papers-to-explore includes research papers on LLMs, NLP, and similar areas. A language model's performance on tasks relevant to text generation and comprehension in the NLP domain can be enhanced by fine-tuning it on such a dataset, which can assist the model acquire field-specific knowledge. About a hundred PDFs make up the set, which offers a sizable amount of text data for optimizing the LLM. Enhancing the language model's ability for language generation and comprehension can be achieved through extensive training on a large sample of data. Through the process of extracting essential concepts, keywords, and phrases from research articles inside the dataset, researchers can experiment with different prompt formats, setups, and techniques to evaluate their effect on LLM performance.

 # Model : LLM ( Llama 2 7B-chat, Langchain)
 Large volumes of text data are used for pre-training LLMs. The model learns from vast text corpora without explicit labels, similar to the dataset given, through the use of unsupervised learning techniques. Llama 2 is based on the Transformer architecture which has a self-attention mechanism that allows the model to process each token individually, weighing the relative relevance of each token in turn. Due to lack of heavy computational resources, I have fine tuned with the Llama 2 7B-chat which is trained on a dataset of 7B words. The dataset has around 10K words so it is suitable for the smaller sized version of Llama 2.

On the other hand, as the dataset is a set of research papers, I needed to utilize a question-answering prompt template and hence Langchain library is used here to create a system like chatbot. It needs a particular formatting that must be followed, such as the INST and <> tags, BOS and EOS tokens, whitespaces, and breaklines, in order to get the necessary features and performance for the chat versions. 

# LLAMA 2 Capabilities and Limitations 
Llama 2 is available in multiple sizes. Its expanded context length of 4,000 tokens enables the model to comprehend and produce large amounts of content more effectively.
Llama 2 7B shows amazing performance, let alone Llama 2 70B one, when compared to closed-source models such as GPT-3.5. 
From my observation, I do realize Llama 2 is heavily dependent on dataset size and quality as it may have overfitting issues with a limited dataset. However, it can always have a potential for further fine tuning such as tuning on the Llama 2 70B model overcoming computational limitations. We need to use LoRA or QLoRA to fine tune on a limited computational power to have better accuracy. 
